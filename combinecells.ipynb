{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "frame_id = 0\n",
    "person_num = 0\n",
    "video_clip = None\n",
    "video_clip_set = []\n",
    "vid_len = len(os.listdir('frames'))\n",
    "while frame_id < vid_len:\n",
    "    frame = cv2.imread('frames/%d.jpg' % frame_id)\n",
    "    if frame is None:\n",
    "        print(\"Warning: Could not read image file: frames/%d.jpg\" % frame_id)\n",
    "        frame_id += 1\n",
    "        continue  # Skip to the next frame\n",
    "    w, h, c = frame.shape\n",
    "    txt_path = 'result/labels/%d.txt' % frame_id\n",
    "    \n",
    "    # Check if the text file exists\n",
    "    face_bbox = []\n",
    "    if os.path.exists(txt_path):\n",
    "        try:\n",
    "            with open(txt_path, 'r') as f:\n",
    "                # Iterate through each line\n",
    "                for line in f.readlines():\n",
    "                    line = line.strip()\n",
    "                    line = line.split(' ')\n",
    "                    for i in range(len(line)):\n",
    "                        line[i] = eval(line[i])\n",
    "                    # Store data from each line into dictionary\n",
    "                    if line[0] == 1:\n",
    "                        face_bbox.append([(line[1]), (line[2]), (line[3]), (line[4])])\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {txt_path}: {e}\")\n",
    "    else:\n",
    "        # File doesn't exist - likely no detections\n",
    "        print(f\"No detections in frame {frame_id} (file {txt_path} not found)\")\n",
    "    \n",
    "    # Sort by first dimension\n",
    "    if face_bbox:  # This will be False if the list is empty\n",
    "        face_bbox = sorted(face_bbox, key=lambda x: x[0])\n",
    "        cur_person_num = len(face_bbox)\n",
    "    else:\n",
    "        cur_person_num = 0\n",
    "    \n",
    "    # Initialize video_clip if it's None\n",
    "    if video_clip is None:\n",
    "        video_clip = {'frame_id': [], 'person_num': cur_person_num}\n",
    "        video_clip['frame_id'].append(frame_id)\n",
    "        for i in range(cur_person_num):\n",
    "            video_clip['p'+str(i)] = [face_bbox[i]]\n",
    "    elif cur_person_num != person_num:\n",
    "        # Person count changed, save current clip and start a new one\n",
    "        video_clip_set.append(video_clip)\n",
    "        video_clip = {'frame_id': [], 'person_num': cur_person_num}\n",
    "        video_clip['frame_id'].append(frame_id)\n",
    "        for i in range(cur_person_num):\n",
    "            video_clip['p'+str(i)] = [face_bbox[i]]\n",
    "    else:\n",
    "        # Same number of people, add to current clip\n",
    "        video_clip['frame_id'].append(frame_id)\n",
    "        for i in range(cur_person_num):\n",
    "            video_clip['p'+str(i)].append(face_bbox[i])\n",
    "    \n",
    "    person_num = cur_person_num\n",
    "    frame_id += 1\n",
    "\n",
    "# Add the last video clip if it exists\n",
    "if video_clip is not None:\n",
    "    video_clip_set.append(video_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.apis import init_detector\n",
    "from mmdet.datasets.pipelines import Compose\n",
    "import torch\n",
    "from mmcv.parallel import collate, scatter\n",
    "import numpy as np\n",
    "model = init_detector(\n",
    "        '/home/s14-htx/Documents/kailin/all_my_stuff/MCGaze/configs/multiclue_gaze/multiclue_gaze_r50_l2cs.py',\n",
    "        '/home/s14-htx/Documents/kailin/all_my_stuff/MCGaze/ckpts/multiclue_gaze_r50_l2cs.pth',\n",
    "        device=\"cuda:0\",\n",
    "        cfg_options=None,)\n",
    "cfg = model.cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfg.data.test.pipeline[1:])\n",
    "test_pipeline = Compose(cfg.data.test.pipeline[1:])\n",
    "\n",
    "def load_datas(data, test_pipeline, datas):\n",
    "    datas.append(test_pipeline(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(datas,model,clip,i):\n",
    "    datas = sorted(datas, key=lambda x:x['img_metas'].data['filename']) # 按帧顺序 img名称从小到大\n",
    "    datas = collate(datas, samples_per_gpu=len(frame_id)) # 用来形成batch用的\n",
    "    datas['img_metas'] = datas['img_metas'].data\n",
    "    datas['img'] = datas['img'].data\n",
    "    datas = scatter(datas, [\"cuda:0\"])[0]\n",
    "    with torch.no_grad():\n",
    "        (det_bboxes, det_labels), det_gazes = model(\n",
    "                return_loss=False,\n",
    "                rescale=True,\n",
    "                format=False,# 返回的bbox既包含face_bboxes也包含head_bboxes\n",
    "                **datas)    # 返回的bbox格式是[x1,y1,x2,y2],根据return_loss函数来判断是forward_train还是forward_test.\n",
    "    gaze_dim = det_gazes['gaze_score'].size(1)\n",
    "    det_fusion_gaze = det_gazes['gaze_score'].view((det_gazes['gaze_score'].shape[0], 1, gaze_dim))\n",
    "    clip['gaze_p'+str(i)].append(det_fusion_gaze.cpu().numpy()) \n",
    "\n",
    "max_len = 100\n",
    "for clip in video_clip_set:\n",
    "    frame_id = clip['frame_id']\n",
    "    person_num = clip['person_num']\n",
    "    for i in range(person_num):\n",
    "        head_bboxes = clip['p'+str(i)]\n",
    "        clip['gaze_p'+str(i)] = []\n",
    "        datas = []\n",
    "        for j,frame in enumerate(frame_id):\n",
    "            cur_img = cv2.imread(\"frames/\"+str(frame)+\".jpg\")\n",
    "            w,h,_ = cur_img.shape\n",
    "            for xy in head_bboxes[j]:\n",
    "                xy = int(xy)\n",
    "            head_center = [int(head_bboxes[j][1]+head_bboxes[j][3])//2,int(head_bboxes[j][0]+head_bboxes[j][2])//2]\n",
    "            l = int(max(head_bboxes[j][3]-head_bboxes[j][1],head_bboxes[j][2]-head_bboxes[j][0])*0.8)\n",
    "            head_crop = cur_img[max(0,head_center[0]-l):min(head_center[0]+l,w),max(0,head_center[1]-l):min(head_center[1]+l,h),:]\n",
    "            w_n,h_n,_ = head_crop.shape\n",
    "            # if frame==0:\n",
    "            #     plt.imshow(head_crop)\n",
    "            # print(head_crop.shape)\n",
    "            cur_data = dict(filename=j,ori_filename=111,img=head_crop,img_shape=(w_n,h_n,3),ori_shape=(2*l,2*l,3),img_fields=['img'])\n",
    "            load_datas(cur_data,test_pipeline,datas)\n",
    "            if len(datas)>max_len or j==(len(frame_id)-1):\n",
    "                infer(datas,model,clip,i)\n",
    "                datas = []\n",
    "                if j==(len(frame_id)-1):\n",
    "                    clip['gaze_p'+str(i)] = np.concatenate(clip['gaze_p'+str(i)],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configuration\n",
    "TOLERANCE_DEG = 114\n",
    "EYE_LEVEL_TOLERANCE = 18\n",
    "PARALLEL_EPSILON = 1e-6          # Threshold for considering lines parallel\n",
    "\n",
    "# Tracking dictionaries\n",
    "eye_contact_log = []  # Format: [frame_id, detected, theta_deg]\n",
    "eye_level_log = []    # Format: [frame_id, detected, min_y_diff]\n",
    "joint_attention_log = []  # Format: [frame_id, detected, point]\n",
    "joint_attention_streaks = defaultdict(lambda: {'count': 0, 'points': []})\n",
    "\n",
    "def check_eye_level(head_centers, tolerance):\n",
    "    for j in range(len(head_centers)):\n",
    "        for k in range(j+1, len(head_centers)):\n",
    "            y_diff = abs(head_centers[j][1] - head_centers[k][1])\n",
    "            if y_diff <= tolerance:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def calculate_intersection(p1, v1, p2, v2):\n",
    "    \"\"\"Calculate intersection point of two gaze vectors\"\"\"\n",
    "    # Check for zero vectors\n",
    "    if (v1[0] == 0 and v1[1] == 0) or (v2[0] == 0 and v2[1] == 0):\n",
    "        return None\n",
    "    \n",
    "    # Calculate determinant\n",
    "    D = v1[1] * v2[0] - v1[0] * v2[1]\n",
    "    if abs(D) < PARALLEL_EPSILON:\n",
    "        return None  # Lines are parallel\n",
    "    \n",
    "    # Calculate t and s parameters\n",
    "    numerator_t = (p2[0] - p1[0]) * (-v2[1]) + v2[0] * (p2[1] - p1[1])\n",
    "    numerator_s = v1[0] * (p2[1] - p1[1]) - v1[1] * (p2[0] - p1[0])\n",
    "    t = numerator_t / D\n",
    "    s = numerator_s / D\n",
    "    \n",
    "    # Check if intersection is in front of both subjects\n",
    "    if t >= 0 and s >= 0:\n",
    "        return (\n",
    "            p1[0] + t * v1[0],\n",
    "            p1[1] + t * v1[1]\n",
    "        )\n",
    "    return None\n",
    "\n",
    "def process_joint_attention(streaks, frame_id):\n",
    "    \"\"\"Process joint attention points - no consecutive frames requirement\"\"\"\n",
    "    detected_points = []\n",
    "    for pair_key in list(streaks.keys()):\n",
    "        streak = streaks[pair_key]\n",
    "        \n",
    "        if len(streak['points']) > 0:\n",
    "            # Just use the most recent point\n",
    "            point = streak['points'][-1]\n",
    "            detected_points.append(point)\n",
    "            joint_attention_log.append([frame_id, True, point])\n",
    "        else:\n",
    "            del streaks[pair_key]\n",
    "    return detected_points\n",
    "\n",
    "for vid_clip in video_clip_set:\n",
    "    for i, frame_id in enumerate(vid_clip['frame_id']):\n",
    "        cur_img = cv2.imread(f\"frames/{vid_clip['frame_id'][i]}.jpg\")\n",
    "        person_count = vid_clip['person_num']\n",
    "        current_gazes = []\n",
    "        head_centers = []\n",
    "\n",
    "        # Collect gaze data\n",
    "        for j in range(person_count):\n",
    "            gaze = vid_clip[f'gaze_p{j}'][i][0]\n",
    "            head_bboxes = [int(xy) for xy in vid_clip[f'p{j}'][i]]\n",
    "            head_center = (\n",
    "                (head_bboxes[0] + head_bboxes[2]) // 2,\n",
    "                (head_bboxes[1] + head_bboxes[3]) // 2\n",
    "            )\n",
    "            current_gazes.append(gaze)\n",
    "            head_centers.append(head_center)\n",
    "\n",
    "        eye_contact_detected = False\n",
    "        max_theta_deg = 0\n",
    "        \n",
    "        # Check all pairs for maximum angle between gaze vectors\n",
    "        for j in range(person_count):\n",
    "            for k in range(j+1, person_count):\n",
    "                u = current_gazes[j]\n",
    "                v = current_gazes[k]\n",
    "                dot_product = u[0]*v[0] + u[1]*v[1]\n",
    "                mag_u = math.hypot(u[0], u[1])\n",
    "                mag_v = math.hypot(v[0], v[1])\n",
    "                \n",
    "                if mag_u * mag_v == 0:\n",
    "                    continue\n",
    "                \n",
    "                cos_theta = dot_product / (mag_u * mag_v)\n",
    "                theta_deg = math.degrees(math.acos(max(min(cos_theta, 1.0), -1.0)))\n",
    "                \n",
    "                # Always update max_theta_deg regardless of threshold\n",
    "                max_theta_deg = max(max_theta_deg, theta_deg)\n",
    "                \n",
    "                # Check if this pair has eye contact\n",
    "                if theta_deg >= (180 - TOLERANCE_DEG):\n",
    "                    eye_contact_detected = True\n",
    "                    \n",
    "        # If no valid pairs were found (e.g., only one person), set max_theta_deg to 0\n",
    "        if person_count < 2 or max_theta_deg == 0:\n",
    "            max_theta_deg = 0\n",
    "\n",
    "        # Detect eye level - always calculate minimum vertical difference\n",
    "        eye_level_detected = False\n",
    "        min_y_diff = float('inf')\n",
    "        \n",
    "        # Check all pairs for minimum vertical difference between heads\n",
    "        if len(head_centers) >= 2:\n",
    "            for j in range(len(head_centers)):\n",
    "                for k in range(j+1, len(head_centers)):\n",
    "                    y_diff = abs(head_centers[j][1] - head_centers[k][1])\n",
    "                    min_y_diff = min(min_y_diff, y_diff)\n",
    "                    \n",
    "                    # Check if this pair is at eye level\n",
    "                    if y_diff <= EYE_LEVEL_TOLERANCE:\n",
    "                        eye_level_detected = True\n",
    "        \n",
    "        # If no valid pairs were found (e.g., only one person), set min_y_diff to -1\n",
    "        if min_y_diff == float('inf'):\n",
    "            min_y_diff = -1\n",
    "            \n",
    "        # Always record both metrics, regardless of detection status\n",
    "        # For eye contact, we track the maximum angle between any gaze vectors\n",
    "        # even if it doesn't exceed our threshold for eye contact detection\n",
    "        eye_contact_log.append([frame_id, eye_contact_detected, max_theta_deg])\n",
    "        \n",
    "        # For eye level, we track the minimum vertical difference between any heads\n",
    "        # even if it doesn't meet our threshold for eye level detection\n",
    "        eye_level_log.append([frame_id, eye_level_detected, min_y_diff])\n",
    "\n",
    "        # Add visual annotations\n",
    "        y_pos = 50\n",
    "        if eye_contact_detected:\n",
    "            cv2.putText(cur_img, \"Eye Contact Detected\", (50, y_pos),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            y_pos += 40\n",
    "            \n",
    "        # Joint attention detection\n",
    "        current_intersections = []\n",
    "        for j in range(person_count):\n",
    "            for k in range(j+1, person_count):\n",
    "                p1 = head_centers[j]\n",
    "                v1 = current_gazes[j]\n",
    "                p2 = head_centers[k]\n",
    "                v2 = current_gazes[k]\n",
    "                \n",
    "                # Define pair_key here first\n",
    "                pair_key = (j, k)\n",
    "                \n",
    "                # Calculate intersection\n",
    "                point = calculate_intersection(p1, v1, p2, v2)\n",
    "                if point:\n",
    "                    # No need to increment count since we're not using it anymore\n",
    "                    joint_attention_streaks[pair_key]['points'] = [point]  # Just keep the current point\n",
    "                    current_intersections.append(point)\n",
    "                else:\n",
    "                    if pair_key in joint_attention_streaks:\n",
    "                        del joint_attention_streaks[pair_key]\n",
    "\n",
    "        # Process valid joint attention\n",
    "        valid_points = process_joint_attention(joint_attention_streaks, frame_id)\n",
    "        has_joint_attention = len(valid_points) > 0\n",
    "        \n",
    "        # Record joint attention for this frame (even if not detected)\n",
    "        if not has_joint_attention:\n",
    "            joint_attention_log.append([frame_id, False, None])\n",
    "\n",
    "        # Visualization\n",
    "        y_pos = 50\n",
    "        # Draw gaze arrows\n",
    "        for j in range(person_count):\n",
    "            gaze = current_gazes[j]\n",
    "            head_center = head_centers[j]\n",
    "            l = int(max(\n",
    "                vid_clip[f'p{j}'][i][3] - vid_clip[f'p{j}'][i][1],\n",
    "                vid_clip[f'p{j}'][i][2] - vid_clip[f'p{j}'][i][0]\n",
    "            )) * 1\n",
    "            gaze_len = l * 1.0\n",
    "            thick = max(5, int(l * 0.01))\n",
    "            \n",
    "            end_point = (\n",
    "                int(head_center[0] - gaze_len * gaze[0]),\n",
    "                int(head_center[1] - gaze_len * gaze[1])\n",
    "            )\n",
    "            cv2.arrowedLine(cur_img, head_center, end_point,\n",
    "                           (230, 253, 11), thickness=thick)\n",
    "\n",
    "        # Save frame\n",
    "        cv2.imwrite(f'new_frames/{frame_id}.jpg', cur_img)\n",
    "\n",
    "# Save combined log with all metrics\n",
    "with open(f'{nameonly}_gaze_behavior_log.txt', 'w') as f:\n",
    "    # Write header\n",
    "    f.write(\"Frame\\tEyeContact\\tEyeContact_Angle\\tEyeLevel\\tEyeLevel_VertDiff\\tJointAttention\\tJA_X\\tJA_Y\\n\")\n",
    "    \n",
    "    # Create a dictionary to organize all data by frame\n",
    "    all_data = {}\n",
    "    \n",
    "    # Process eye contact data\n",
    "    for frame_id, detected, theta_deg in eye_contact_log:\n",
    "        if frame_id not in all_data:\n",
    "            all_data[frame_id] = {\"eye_contact\": (detected, theta_deg)}\n",
    "        else:\n",
    "            all_data[frame_id][\"eye_contact\"] = (detected, theta_deg)\n",
    "    \n",
    "    # Process eye level data\n",
    "    for frame_id, detected, y_diff in eye_level_log:\n",
    "        if frame_id not in all_data:\n",
    "            all_data[frame_id] = {\"eye_level\": (detected, y_diff)}\n",
    "        else:\n",
    "            all_data[frame_id][\"eye_level\"] = (detected, y_diff)\n",
    "    \n",
    "    # Process joint attention data\n",
    "    for frame_id, detected, point in joint_attention_log:\n",
    "        if frame_id not in all_data:\n",
    "            all_data[frame_id] = {\"joint_attention\": (detected, point)}\n",
    "        else:\n",
    "            all_data[frame_id][\"joint_attention\"] = (detected, point)\n",
    "    \n",
    "    # Write all data in frame order\n",
    "    for frame_id in sorted(all_data.keys()):\n",
    "        data = all_data[frame_id]\n",
    "        \n",
    "        # Default values if data is missing\n",
    "        ec_detected = \"False\"\n",
    "        ec_angle = \"N/A\"\n",
    "        el_detected = \"False\"\n",
    "        el_diff = \"N/A\"\n",
    "        ja_detected = \"False\"\n",
    "        ja_x = \"N/A\"\n",
    "        ja_y = \"N/A\"\n",
    "        \n",
    "        # Fill in eye contact data if available\n",
    "        if \"eye_contact\" in data:\n",
    "            ec_detected, ec_angle = data[\"eye_contact\"]\n",
    "            # Always format the angle, even if eye contact wasn't detected\n",
    "            ec_angle = f\"{ec_angle:.2f}\" if isinstance(ec_angle, (int, float)) else \"0.00\"\n",
    "        \n",
    "        # Fill in eye level data if available\n",
    "        if \"eye_level\" in data:\n",
    "            el_detected, el_diff = data[\"eye_level\"]\n",
    "            # Ensure we have a numeric value for vertical difference\n",
    "            el_diff = el_diff if el_diff != -1 else \"N/A\"\n",
    "        \n",
    "        # Fill in joint attention data if available\n",
    "        if \"joint_attention\" in data:\n",
    "            ja_detected, point = data[\"joint_attention\"]\n",
    "            if ja_detected and point:\n",
    "                ja_x = f\"{point[0]:.1f}\"\n",
    "                ja_y = f\"{point[1]:.1f}\"\n",
    "        \n",
    "        # Write the line\n",
    "        f.write(f\"{frame_id}\\t{ec_detected}\\t{ec_angle}\\t{el_detected}\\t{el_diff}\\t{ja_detected}\\t{ja_x}\\t{ja_y}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def majority_filter(binary_array, window_size=5):\n",
    "    \"\"\"\n",
    "    Apply a majority filter to smooth binary detection results.\n",
    "    This will convert patterns like tttfttt to ttttttt.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    binary_array : array-like\n",
    "        Array of boolean values to smooth\n",
    "    window_size : int\n",
    "        Size of the sliding window (should be odd)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Smoothed boolean array\n",
    "    \"\"\"\n",
    "    smoothed = np.copy(binary_array)\n",
    "    half_window = window_size // 2\n",
    "    \n",
    "    for i in range(len(binary_array)):\n",
    "        # Define window boundaries with edge handling\n",
    "        start = max(0, i - half_window)\n",
    "        end = min(len(binary_array), i + half_window + 1)\n",
    "        \n",
    "        # Count True values in the window\n",
    "        window = binary_array[start:end]\n",
    "        true_count = np.sum(window)\n",
    "        \n",
    "        # Set to True if majority are True\n",
    "        smoothed[i] = true_count > (end - start) / 2\n",
    "        \n",
    "    return smoothed\n",
    "\n",
    "def minimum_duration_filter(binary_array, min_duration=15):\n",
    "    \"\"\"\n",
    "    Filter out brief state changes that don't persist for min_duration frames.\n",
    "    This will convert patterns like ffftfff to fffffff.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    binary_array : array-like\n",
    "        Array of boolean values to filter\n",
    "    min_duration : int\n",
    "        Minimum number of consecutive frames required to maintain a state\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Filtered boolean array\n",
    "    \"\"\"\n",
    "    smoothed = np.copy(binary_array)\n",
    "    \n",
    "    # Find runs of True states\n",
    "    i = 0\n",
    "    while i < len(binary_array):\n",
    "        if binary_array[i]:\n",
    "            # Found a True state, check its duration\n",
    "            start = i\n",
    "            while i < len(binary_array) and binary_array[i]:\n",
    "                i += 1\n",
    "            \n",
    "            duration = i - start\n",
    "            \n",
    "            # If duration is less than minimum and surrounded by False states\n",
    "            if duration < min_duration:\n",
    "                # Check if it's an isolated island (surrounded by False)\n",
    "                is_isolated = (start == 0 or not binary_array[start-1]) and \\\n",
    "                             (i == len(binary_array) or not binary_array[i])\n",
    "                \n",
    "                if is_isolated:\n",
    "                    # Fill with False values\n",
    "                    smoothed[start:i] = [False] * duration\n",
    "        else:\n",
    "            i += 1\n",
    "            \n",
    "    return smoothed\n",
    "\n",
    "def smooth_eye_detection( df, min_duration, window_size):\n",
    "    \n",
    "\n",
    "    columns_to_smooth = ['EyeContact', 'EyeLevel', 'JointAttention']\n",
    "    \n",
    "    # Apply filters to each column\n",
    "    for col in columns_to_smooth:\n",
    "        if col in df.columns:\n",
    "            # Convert column to boolean if it's not already\n",
    "            binary_array = df[col].astype(bool).values\n",
    "            \n",
    "            # First apply minimum duration filter to eliminate brief True sequences\n",
    "            filtered = minimum_duration_filter(binary_array, min_duration)\n",
    "            \n",
    "            # Then apply majority filter to smooth out brief False interruptions\n",
    "            smoothed = majority_filter(filtered, window_size)\n",
    "            \n",
    "            # Create a new column with the smoothed values\n",
    "            df[f'{col}_Smoothed'] = smoothed\n",
    "            \n",
    "            # Print statistics about changes\n",
    "            changes = np.sum(binary_array != smoothed)\n",
    "            # print(f\"Applied smoothing to {col}: {changes} values changed ({changes/len(binary_array):.2%})\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def filter_short_interactions(df, fps=30,smooth=bool, eye_contact_min_seconds=1, joint_attention_min_seconds=3):\n",
    "    \"\"\"\n",
    "    Filter out eye contact sections shorter than the specified duration and\n",
    "    joint attention sections shorter than the specified duration.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing interaction data with SourceFile, Frame, EyeContact_Smoothed, and JointAttention_Smoothed columns\n",
    "    fps : int\n",
    "        Frames per second of the videos (default: 30)\n",
    "    eye_contact_min_seconds : float\n",
    "        Minimum duration in seconds for eye contact sections to keep (default: 1)\n",
    "    joint_attention_min_seconds : float\n",
    "        Minimum duration in seconds for joint attention sections to keep (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with short interactions filtered out\n",
    "    \"\"\"\n",
    "    # Calculate minimum frames for each interaction type\n",
    "    eye_contact_min_frames = int(eye_contact_min_seconds * fps)\n",
    "    joint_attention_min_frames = int(joint_attention_min_seconds * fps)\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    filtered_df = df.copy()\n",
    "    if smooth==True:\n",
    "        eyecontact='EyeContact_Smoothed'\n",
    "        jointattention='JointAttention_Smoothed'\n",
    "    else:\n",
    "        eyecontact='EyeContact'\n",
    "        jointattention='JointAttention'\n",
    "    # Make sure the interaction columns exist\n",
    "    if eyecontact not in filtered_df.columns:\n",
    "        print(\"Warning: 'EyeContact' column not found in the DataFrame\")\n",
    "    if 'JointAttention_Smoothed' not in filtered_df.columns:\n",
    "        print(\"Warning: 'JointAttention_Smoothed' column not found in the DataFrame\")\n",
    "    \n",
    "    # Process each video separately\n",
    "    for source_file, video_df in filtered_df.groupby('SourceFile'):\n",
    "        # print(f\"Processing video: {source_file}\")\n",
    "        \n",
    "        # Get indices for this video group to update the original DataFrame\n",
    "        video_indices = video_df.index\n",
    "        \n",
    "        # Sort by frame number to ensure temporal order\n",
    "        video_df = video_df.sort_values('Frame')\n",
    "        \n",
    "        # Process eye contact interactions\n",
    "        if eyecontact in video_df.columns:\n",
    "            # Convert to numpy array for faster processing\n",
    "            eye_contact_array = video_df[eyecontact].values.astype(bool)\n",
    "            \n",
    "            # Find continuous sections of eye contact\n",
    "            sections = []\n",
    "            in_section = False\n",
    "            start_idx = 0\n",
    "            \n",
    "            for i in range(len(eye_contact_array)):\n",
    "                if eye_contact_array[i] and not in_section:\n",
    "                    # Start of a new section\n",
    "                    in_section = True\n",
    "                    start_idx = i\n",
    "                elif not eye_contact_array[i] and in_section:\n",
    "                    # End of a section\n",
    "                    in_section = False\n",
    "                    duration = i - start_idx\n",
    "                    sections.append((start_idx, i, duration))\n",
    "            \n",
    "            # Handle case where the last section extends to the end\n",
    "            if in_section:\n",
    "                duration = len(eye_contact_array) - start_idx\n",
    "                sections.append((start_idx, len(eye_contact_array), duration))\n",
    "            \n",
    "            # Filter out short sections\n",
    "            for start, end, duration in sections:\n",
    "                if duration < eye_contact_min_frames:\n",
    "                    # Set this section to False\n",
    "                    eye_contact_array[start:end] = False\n",
    "            \n",
    "            # Update the DataFrame with filtered array\n",
    "            filtered_df.loc[video_indices, eyecontact] = eye_contact_array\n",
    "        \n",
    "        # Process joint attention interactions\n",
    "        if jointattention in video_df.columns:\n",
    "            # Convert to numpy array for faster processing\n",
    "            joint_att_array = video_df[jointattention].values.astype(bool)\n",
    "            \n",
    "            # Find continuous sections of joint attention\n",
    "            sections = []\n",
    "            in_section = False\n",
    "            start_idx = 0\n",
    "            \n",
    "            for i in range(len(joint_att_array)):\n",
    "                if joint_att_array[i] and not in_section:\n",
    "                    # Start of a new section\n",
    "                    in_section = True\n",
    "                    start_idx = i\n",
    "                elif not joint_att_array[i] and in_section:\n",
    "                    # End of a section\n",
    "                    in_section = False\n",
    "                    duration = i - start_idx\n",
    "                    sections.append((start_idx, i, duration))\n",
    "            \n",
    "            # Handle case where the last section extends to the end\n",
    "            if in_section:\n",
    "                duration = len(joint_att_array) - start_idx\n",
    "                sections.append((start_idx, len(joint_att_array), duration))\n",
    "            \n",
    "            # Filter out short sections\n",
    "            for start, end, duration in sections:\n",
    "                if duration < joint_attention_min_frames:\n",
    "                    # Set this section to False\n",
    "                    joint_att_array[start:end] = False\n",
    "            \n",
    "            # Update the DataFrame with filtered array\n",
    "            filtered_df.loc[video_indices, jointattention] = joint_att_array\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(f'{nameonly}_gaze_behavior_log.txt', sep='\\t')\n",
    "\n",
    "smooth=True\n",
    "window_size=5\n",
    "min_duration=15\n",
    "\n",
    "# df= adjust_angles(df, smooth, type)\n",
    "df= smooth_eye_detection( df, min_duration, window_size)\n",
    "filtered_df = filter_short_interactions(\n",
    "    df,\n",
    "    fps=30,  # Adjust based on your video frame rate\n",
    "    eye_contact_min_seconds=1,\n",
    "    joint_attention_min_seconds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_interactions_to_frames(df, nameonly fps=30):\n",
    "    \"\"\"\n",
    "    Add detected interaction texts to frames and create a video.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        DataFrame with interaction data\n",
    "    fps : int\n",
    "        Frames per second for the output video\n",
    "    \"\"\"\n",
    "    interaction_columns = ['EyeContact_Smoothed', 'EyeLevel_Smoothed', 'JointAttention_Smoothed']\n",
    "    \n",
    "    # Convert interaction columns to boolean if they exist\n",
    "    for col in interaction_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.lower() == 'true'\n",
    "    \n",
    "    img = cv2.imread('new_frames/0.jpg')  #读取第一张图片\n",
    "    fps = 30\n",
    "    imgInfo = img.shape\n",
    "    size = (imgInfo[1],imgInfo[0]) \n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    videoWrite = cv2.VideoWriter(f'{nameonly}_results.mp4',fourcc,fps,size)# 根据图片的大小，创建写入对象 （文件名，支持的编码器，25帧，视频大小（图片大小））\n",
    " \n",
    "    frame_to_interactions = {}\n",
    "    for _, row in df.iterrows():\n",
    "        frame_num = int(row['Frame'])\n",
    "        interactions = []\n",
    "        \n",
    "        if 'EyeContact_Smoothed' in row and row['EyeContact_Smoothed']:\n",
    "            interactions.append(\"Eye Contact\")\n",
    "        \n",
    "        if 'EyeLevel_Smoothed' in row and row['EyeLevel_Smoothed']:\n",
    "            interactions.append(\"Eye Level\")\n",
    "        \n",
    "        if 'JointAttention_Smoothed' in row and row['JointAttention_Smoothed']:\n",
    "            interactions.append(\"Joint Attention\")\n",
    "        \n",
    "        frame_to_interactions[frame_num] = interactions\n",
    "    \n",
    "        frame_count = len(os.listdir('new_frames/'))\n",
    "        print(f\"Processing {frame_count} frames...\")\n",
    "    for i, frame_file in range(0, frame_count):\n",
    "        # Extract frame number from filename\n",
    "        frame_num = int(frame_file.split('.')[0])\n",
    "        \n",
    "        # Read the frame\n",
    "        file_path = 'new_frames/'+str(i)+'.jpg'    #循环读取所有的图片,假设以数字顺序命名\n",
    "        frame = cv2.imread(file_path)\n",
    "        \n",
    "        if frame is None:\n",
    "            print(f\"Error reading frame: {file_path}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Get interactions for this frame if available\n",
    "        interactions = frame_to_interactions.get(frame_num, [])\n",
    "        \n",
    "        # Add text overlays for each interaction\n",
    "        y_position = 40  # Starting y-position for text\n",
    "        for interaction in interactions:\n",
    "            # Get text size for background rectangle\n",
    "            text_size = cv2.getTextSize(\n",
    "                interaction,\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                1,  # font scale\n",
    "                2   # thickness\n",
    "            )[0]\n",
    "            \n",
    "            # Draw black background rectangle\n",
    "            cv2.rectangle(\n",
    "                frame,\n",
    "                (10, y_position - 30),  # top-left corner\n",
    "                (10 + text_size[0], y_position + 10),  # bottom-right corner\n",
    "                (0, 0, 0),  # color (black)\n",
    "                -1  # filled rectangle\n",
    "            )\n",
    "            \n",
    "            # Draw text\n",
    "            cv2.putText(\n",
    "                frame, \n",
    "                interaction, \n",
    "                (10, y_position),  # position (x, y)\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,  # font\n",
    "                1,  # font scale\n",
    "                (0, 255, 0),  # color (green)\n",
    "                2,  # thickness\n",
    "                cv2.LINE_AA  # line type\n",
    "            )\n",
    "            \n",
    "            # Increment y-position for next text with enough space to avoid overlap\n",
    "            y_position += text_size[1] + 20  # Add text height plus padding\n",
    "        \n",
    "        # Write the annotated frame to output video\n",
    "        videoWrite.write(frame)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_interactions_to_frames(\n",
    "    filtered_df, nameonly\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
